
@inproceedings{Klower2019,
	address = {Singapore, Singapore},
	title = {Posits as an alternative to floats for weather and climate models},
	isbn = {978-1-4503-7139-1},
	url = {http://dl.acm.org/citation.cfm?doid=3316279.3316281},
	doi = {10.1145/3316279.3316281},
	abstract = {Posit numbers, a recently proposed alternative to floating-point numbers, claim to have smaller arithmetic rounding errors in many applications. By studying weather and climate models of low and medium complexity (the Lorenz system and a shallow water model) we present benefits of posits compared to floats at 16 bit. As a standardised posit processor does not exist yet, we emulate posit arithmetic on a conventional CPU. Using a shallow water model, forecasts based on 16-bit posits with 1 or 2 exponent bits are clearly more accurate than half precision floats. We therefore propose 16 bit with 2 exponent bits as a standard posit format, as its wide dynamic range of 32 orders of magnitude provides a great potential for many weather and climate models. Although the focus is on geophysical fluid simulations, the results are also meaningful and promising for reduced precision posit arithmetic in the wider field of computational fluid dynamics.},
	language = {en},
	urldate = {2020-02-25},
	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic} 2019 on   - {CoNGA}'19},
	publisher = {ACM Press},
	author = {Klöwer, Milan and Düben, Peter D. and Palmer, Tim N.},
	year = {2019},
	pages = {1--8},
	file = {Klöwer, Düben, Palmer_2019.pdf:/home/kloewer/papers/Klöwer, Düben, Palmer_2019.pdf:application/pdf}
}

@article{Diamantakis2013,
	title = {The semi-{Lagrangian} technique in atmospheric modelling: current status and future challenges},
	abstract = {The semi-Lagrangian method is an established numerical technique for integrating the transport equations in atmospheric models. Coupled with semi-implicit time-stepping offers unconditional stability for all forcing terms in equation sets of such models. This distinct advantage has led to the development of very efﬁcient numerical weather prediction systems such as the ECMWF Integrated Forecasting System (IFS).},
	language = {en},
	author = {Diamantakis, Michail},
	year = {2013},
	pages = {18},
	file = {Diamantakis_2013.pdf:/home/kloewer/papers/Diamantakis_2013.pdf:application/pdf}
}

@article{Johnson2018,
	title = {Rethinking floating point for deep learning},
	url = {http://arxiv.org/abs/1811.01721},
	abstract = {Reducing hardware overhead of neural networks for faster or lower power inference and training is an active area of research. Uniform quantization using integer multiply-add has been thoroughly investigated, which requires learning many quantization parameters, ﬁne-tuning training or other prerequisites. Little effort is made to improve ﬂoating point relative to this baseline; it remains energy inefﬁcient, and word size reduction yields drastic loss in needed dynamic range. We improve ﬂoating point to be more energy efﬁcient than equivalent bit width integer hardware on a 28 nm ASIC process while retaining accuracy in 8 bits with a novel hybrid log multiply/linear add, Kulisch accumulation and tapered encodings from Gustafson’s posit format. With no network retraining, and drop-in replacement of all math and ﬂoat32 parameters via round-to-nearest-even only, this open-sourced 8-bit log ﬂoat is within 0.9\% top-1 and 0.2\% top-5 accuracy of the original ﬂoat32 ResNet-50 CNN model on ImageNet. Unlike int8 quantization, it is still a general purpose ﬂoating point arithmetic, interpretable out-of-the-box. Our 8/38-bit log ﬂoat multiply-add is synthesized and power proﬁled at 28 nm at 0.96× the power and 1.12× the area of 8/32-bit integer multiply-add. In 16 bits, our log ﬂoat multiply-add is 0.59× the power and 0.68× the area of IEEE 754 ﬂoat16 fused multiply-add, maintaining the same signﬁcand precision and dynamic range, proving useful for training ASICs as well.},
	language = {en},
	urldate = {2020-02-25},
	journal = {arXiv:1811.01721 [cs]},
	author = {Johnson, Jeff},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.01721},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {Johnson_Unknown.pdf:/home/kloewer/papers/Johnson_Unknown.pdf:application/pdf}
}

@article{Russell2017,
	title = {Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures},
	volume = {221},
	issn = {00104655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465517302564},
	doi = {10.1016/j.cpc.2017.08.011},
	abstract = {Reconfigurable architectures are becoming mainstream: Amazon, Microsoft and IBM are supporting such architectures in their data centres. The computationally intensive nature of atmospheric modelling is an attractive target for hardware acceleration using reconfigurable computing. Performance of hardware designs can be improved through the use of reduced-precision arithmetic, but maintaining appropriate accuracy is essential. We explore reduced-precision optimisation for simulating chaotic systems, targeting atmospheric modelling, in which even minor changes in arithmetic behaviour will cause simulations to diverge quickly. The possibility of equally valid simulations having differing outcomes means that standard techniques for comparing numerical accuracy are inappropriate. We use the Hellinger distance to compare statistical behaviour between reduced-precision CPU implementations to guide reconfigurable designs of a chaotic system, then analyse accuracy, performance and power efficiency of the resulting implementations. Our results show that with only a limited loss in accuracy corresponding to less than 10\% uncertainty in input parameters, the throughput and energy efficiency of a single-precision chaotic system implemented on a Xilinx Virtex-6 SX475T Field Programmable Gate Array (FPGA) can be more than doubled.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Computer Physics Communications},
	author = {Russell, Francis P. and Düben, Peter D. and Niu, Xinyu and Luk, Wayne and Palmer, T.N.},
	month = dec,
	year = {2017},
	pages = {160--173},
	file = {Russell et al._2017.pdf:/home/kloewer/papers/Russell et al._2017.pdf:application/pdf}
}

@article{Hatfield2018,
	title = {Choosing the {Optimal} {Numerical} {Precision} for {Data} {Assimilation} in the {Presence} of {Model} {Error}},
	volume = {10},
	issn = {1942-2466, 1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2018MS001341},
	doi = {10.1029/2018MS001341},
	abstract = {The use of reduced numerical precision within an atmospheric data assimilation system is investigated. An atmospheric model with a spectral dynamical core is used to generate synthetic observations, which are then assimilated back into the same model using an ensemble Kalman ﬁlter. The eﬀect on the analysis error of reducing precision from 64 bits to only 22 bits is measured and found to depend strongly on the degree of model uncertainty within the system. When the model used to generate the observations is identical to the model used to assimilate observations, the reduced-precision results suﬀer substantially. However, when model error is introduced by changing the diﬀusion scheme in the assimilation model or by using a higher-resolution model to generate observations, the diﬀerence in analysis quality between the two levels of precision is almost eliminated. Lower-precision arithmetic has a lower computational cost, so lowering precision could free up computational resources in operational data assimilation and allow an increase in ensemble size or grid resolution.},
	language = {en},
	number = {9},
	urldate = {2020-02-25},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Hatfield, Sam and Düben, Peter and Chantry, Matthew and Kondo, Keiichi and Miyoshi, Takemasa and Palmer, Tim},
	month = sep,
	year = {2018},
	pages = {2177--2191},
	file = {Hatfield et al._2018.pdf:/home/kloewer/papers/Hatfield et al._2018.pdf:application/pdf}
}

@inproceedings{Chaurasiya2018,
	address = {Orlando, FL, USA},
	title = {Parameterized {Posit} {Arithmetic} {Hardware} {Generator}},
	isbn = {978-1-5386-8477-1},
	url = {https://ieeexplore.ieee.org/document/8615707/},
	doi = {10.1109/ICCD.2018.00057},
	abstract = {Hardware implementation of Floating Point Units (FPUs) has been a key area of research due to their massive area and energy footprints. Recently, a proposal was made to replace IEEE 754-2008 technical standard compliant FPUs with Posit Arithmetic Units (PAUs) due to the greater accuracy, speed, and simpler hardware design. In this paper, we present the architecture of a parameterized PAU generator that can generate PAU adders and PAU multipliers of any bit-width pre-synthesis. We synthesize generated arithmetic units using the parameterized PAU generator for 8-bit, 16-bit, and 32-bit adders and multipliers and compare them with IEEE 754-2008 compliant adders and multipliers. Both, synthesis for Field Programmable Gate Array (FPGA) and Application Speciﬁc Integrated Circuit (ASIC) are performed. In our comparison of m-bit PAU units with n-bit IEEE 754-2008 compliant units, it is observed that the area and energy of a PAU adder and multiplier are comparable to their IEEE 754-2008 compliant counterparts where m = n. We argue that an n-bit IEEE 754-2008 adder and multiplier can be safely replaced with an m-bit PAU adder and multiplier where m {\textless} n, due to superior numerical accuracy of the PAU; we also compare m-bit PAU adders and multipliers with n-bit IEEE 754-2008 compliant adders and multipliers. As an application example, we examine performance in the domain of signal processing with and without PAU adders and multipliers, and show the advantage of our approach.},
	language = {en},
	urldate = {2020-02-25},
	booktitle = {2018 {IEEE} 36th {International} {Conference} on {Computer} {Design} ({ICCD})},
	publisher = {IEEE},
	author = {Chaurasiya, Rohit and Gustafson, John and Shrestha, Rahul and Neudorfer, Jonathan and Nambiar, Sangeeth and Niyogi, Kaustav and Merchant, Farhad and Leupers, Rainer},
	month = oct,
	year = {2018},
	pages = {334--341},
	file = {Chaurasiya et al._2018.pdf:/home/kloewer/papers/Chaurasiya et al._2018.pdf:application/pdf}
}

@article{Glaser2017,
	title = {An 826 {MOPS}, 210 {uW}/{MHz} {Unum} {ALU} in 65 nm},
	url = {http://arxiv.org/abs/1712.01021},
	abstract = {To overcome the limitations of conventional ﬂoatingpoint number formats, an interval arithmetic and variablewidth storage format called universal number (unum) has been recently introduced [1]. This paper presents the ﬁrst (to the best of our knowledge) silicon implementation measurements of an application-speciﬁc integrated circuit (ASIC) for unum ﬂoating-point arithmetic. The designed chip includes a 128-bit wide unum arithmetic unit to execute additions and subtractions, while also supporting lossless (for intermediate results) and lossy (for external data movements) compression units to exploit the memory usage reduction potential of the unum format. Our chip, fabricated in a 65 nm CMOS process, achieves a maximum clock frequency of 413 MHz at 1.2 V with an average measured power of 210 uW/MHz.},
	language = {en},
	urldate = {2020-02-25},
	journal = {arXiv:1712.01021 [cs]},
	author = {Glaser, Florian and Mach, Stefan and Rahimi, Abbas and Gürkaynak, Frank K. and Huang, Qiuting and Benini, Luca},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01021},
	keywords = {Computer Science - Hardware Architecture},
	file = {Glaser et al._2017.pdf:/home/kloewer/papers/Glaser et al._2017.pdf:application/pdf}
}

@article{Kwasniok2014,
	title = {Enhanced regime predictability in atmospheric low-order models due to stochastic forcing},
	volume = {372},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2013.0286},
	doi = {10.1098/rsta.2013.0286},
	language = {en},
	number = {2018},
	urldate = {2020-02-25},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Kwasniok, Frank},
	month = jun,
	year = {2014},
	pages = {20130286},
	file = {Kwasniok_2014.pdf:/home/kloewer/papers/Kwasniok_2014.pdf:application/pdf}
}

@article{Tantet2018,
	title = {Resonances in a {Chaotic} {Attractor} {Crisis} of the {Lorenz} {Flow}},
	volume = {170},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-017-1938-0},
	doi = {10.1007/s10955-017-1938-0},
	abstract = {Local bifurcations of stationary points and limit cycles have successfully been characterized in terms of the critical exponents of these solutions. Lyapunov exponents and their associated covariant Lyapunov vectors have been proposed as tools for supporting the understanding of critical transitions in chaotic dynamical systems. However, it is in general not clear how the statistical properties of dynamical systems change across a boundary crisis during which a chaotic attractor collides with a saddle. This behavior is investigated here for a boundary crisis in the Lorenz ﬂow, for which neither the Lyapunov exponents nor the covariant Lyapunov vectors provide a criterion for the crisis. Instead, the convergence of the time evolution of probability densities to the invariant measure, governed by the semigroup of transfer operators, is expected to slow down at the approach of the crisis. Such convergence is described by the eigenvalues of the generator of this semigroup, which can be divided into two families, referred to as the stable and unstable Ruelle–Pollicott resonances, respectively. The former describes the convergence of densities to the attractor (or escape from a repeller) and is estimated from many short time series sampling the state space. The latter is responsible for the decay of correlations, or mixing, and can be estimated from a long times series, invoking ergodicity. It is found numerically for the Lorenz ﬂow that the stable resonances do approach the imaginary axis during the crisis, as is indicative of the loss of global stability of the attractor. On the other hand, the unstable resonances, and a fortiori the decay of correlations, do not ﬂag the proximity of the crisis, thus questioning the usual design of early warning indicators of boundary crises of chaotic attractors and the applicability of response theory close to such crises.},
	language = {en},
	number = {3},
	urldate = {2020-02-25},
	journal = {Journal of Statistical Physics},
	author = {Tantet, Alexis and Lucarini, Valerio and Dijkstra, Henk A.},
	month = feb,
	year = {2018},
	pages = {584--616},
	file = {Tantet, Lucarini, Dijkstra_2018.pdf:/home/kloewer/papers/Tantet, Lucarini, Dijkstra_2018.pdf:application/pdf}
}

@article{Duben2018,
	title = {A {New} {Number} {Format} for {Ensemble} {Simulations}},
	volume = {10},
	issn = {1942-2466, 1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2018MS001420},
	doi = {10.1029/2018MS001420},
	language = {en},
	number = {11},
	urldate = {2020-02-25},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Düben, Peter D.},
	month = nov,
	year = {2018},
	pages = {2983--2991},
	file = {Düben_2018.pdf:/home/kloewer/papers/Düben_2018.pdf:application/pdf}
}

@article{Dawson2017,
	title = {rpe v5: an emulator for reduced floating-point precision in large numerical simulations},
	volume = {10},
	issn = {1991-9603},
	shorttitle = {rpe v5},
	url = {https://www.geosci-model-dev.net/10/2221/2017/},
	doi = {10.5194/gmd-10-2221-2017},
	abstract = {This paper describes the rpe (reduced-precision emulator) library which has the capability to emulate the use of arbitrary reduced ﬂoating-point precision within large numerical models written in Fortran. The rpe software allows model developers to test how reduced ﬂoating-point precision affects the result of their simulations without having to make extensive code changes or port the model onto specialized hardware. The software can be used to identify parts of a program that are problematic for numerical precision and to guide changes to the program to allow a stronger reduction in precision.},
	language = {en},
	number = {6},
	urldate = {2020-02-25},
	journal = {Geoscientific Model Development},
	author = {Dawson, Andrew and Düben, Peter D.},
	month = jun,
	year = {2017},
	pages = {2221--2230},
	file = {Dawson, Düben_2017.pdf:/home/kloewer/papers/Dawson, Düben_2017.pdf:application/pdf}
}

@article{Thornes2017,
	title = {On the use of scale-dependent precision in {Earth} {System} modelling: {Scale}-{Dependent} {Precision} in {Earth} {System} {Modelling}},
	volume = {143},
	issn = {00359009},
	shorttitle = {On the use of scale-dependent precision in {Earth} {System} modelling},
	url = {http://doi.wiley.com/10.1002/qj.2974},
	doi = {10.1002/qj.2974},
	language = {en},
	number = {703},
	urldate = {2020-02-25},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Thornes, Tobias and Düben, Peter and Palmer, Tim},
	month = jan,
	year = {2017},
	pages = {897--908},
	file = {Thornes, Düben, Palmer_2017.pdf:/home/kloewer/papers/Thornes, Düben, Palmer_2017.pdf:application/pdf}
}

@article{Duben2014,
	title = {The use of imprecise processing to improve accuracy in weather \& climate prediction},
	volume = {271},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999113007183},
	doi = {10.1016/j.jcp.2013.10.042},
	abstract = {The use of stochastic processing hardware and low precision arithmetic in atmospheric models is investigated. Stochastic processors allow hardware-induced faults in calculations, sacriﬁcing bit-reproducibility and precision in exchange for improvements in performance and potentially accuracy of forecasts, due to a reduction in power consumption that could allow higher resolution. A similar trade-off is achieved using low precision arithmetic, with improvements in computation and communication speed and savings in storage and memory requirements. As high-performance computing becomes more massively parallel and power intensive, these two approaches may be important stepping stones in the pursuit of global cloud-resolving atmospheric modelling.},
	language = {en},
	urldate = {2020-02-25},
	journal = {Journal of Computational Physics},
	author = {Düben, Peter D. and McNamara, Hugh and Palmer, T.N.},
	month = aug,
	year = {2014},
	pages = {2--18},
	file = {Düben, McNamara, Palmer_2014.pdf:/home/kloewer/papers/Düben, McNamara, Palmer_2014.pdf:application/pdf}
}

@article{Duben2014a,
	title = {Benchmark {Tests} for {Numerical} {Weather} {Forecasts} on {Inexact} {Hardware}},
	volume = {142},
	issn = {0027-0644, 1520-0493},
	url = {http://journals.ametsoc.org/doi/10.1175/MWR-D-14-00110.1},
	doi = {10.1175/MWR-D-14-00110.1},
	abstract = {A reduction of computational cost would allow higher resolution in numerical weather predictions within the same budget for computation. This paper investigates two approaches that promise signiﬁcant savings in computational cost: the use of reduced precision hardware, which reduces ﬂoating point precision beyond the standard double- and single-precision arithmetic, and the use of stochastic processors, which allow hardware faults in a trade-off between reduced precision and savings in power consumption and computing time. Reduced precision is emulated within simulations of a spectral dynamical core of a global atmosphere model and a detailed study of the sensitivity of different parts of the model to inexact hardware is performed. Afterward, benchmark simulations were performed for which as many parts of the model as possible were put onto inexact hardware. Results show that large parts of the model could be integrated with inexact hardware at error rates that are surprisingly high or with reduced precision to only a couple of bits in the signiﬁcand of ﬂoating point numbers. However, the sensitivities to inexact hardware of different parts of the model need to be respected, for example, via scale separation. In the last part of the paper, simulations with a full operational weather forecast model in single precision are presented. It is shown that differences in accuracy between the single- and double-precision forecasts are smaller than differences between ensemble members of the ensemble forecast at the resolution of the standard ensemble forecasting system. The simulations prove that the trade-off between precision and performance is a worthwhile effort, already on existing hardware.},
	language = {en},
	number = {10},
	urldate = {2020-02-25},
	journal = {Monthly Weather Review},
	author = {Düben, Peter D. and Palmer, T. N.},
	month = oct,
	year = {2014},
	pages = {3809--3829},
	file = {Düben, Palmer_2014.pdf:/home/kloewer/papers/Düben, Palmer_2014.pdf:application/pdf}
}

@inproceedings{Chen2018,
	address = {Singapore, Singapore},
	title = {A matrix-multiply unit for posits in reconfigurable logic leveraging (open){CAPI}},
	isbn = {978-1-4503-6414-0},
	url = {http://dl.acm.org/citation.cfm?doid=3190339.3190340},
	doi = {10.1145/3190339.3190340},
	abstract = {In this paper, we present the design in reconfigurable logic of a matrix multiplier for matrices of 32-bit posit numbers with es=2 [1]. Vector dot products are computed without intermediate rounding as suggested by the proposed posit standard to maximally retain precision. An initial implementation targets the CAPI 1.0 interface on the POWER8 processor and achieves about 10Gpops (Giga posit operations per second). Follow-on implementations targeting CAPI 2.0 and OpenCAPI 3.0 on POWER9 are expected to achieve up to 64Gpops. Our design is available under a permissive open source license at https://github.com/ChenJianyunp/Unum\_matrix\_multiplier. We hope the current work, which works on CAPI 1.0, along with future community contributions, will help enable a more extensive exploration of this proposed new format.},
	language = {en},
	urldate = {2020-02-25},
	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic} on - {CoNGA} '18},
	publisher = {ACM Press},
	author = {Chen, Jianyu and Al-Ars, Zaid and Hofstee, H. Peter},
	year = {2018},
	pages = {1--5},
	file = {Chen, Hofstee_2018.pdf:/home/kloewer/papers/Chen, Hofstee_2018.pdf:application/pdf}
}

@inproceedings{Targett2015,
	address = {Queenstown, New Zealand},
	title = {Lower precision for higher accuracy: {Precision} and resolution exploration for shallow water equations},
	isbn = {978-1-4673-9091-0},
	shorttitle = {Lower precision for higher accuracy},
	url = {http://ieeexplore.ieee.org/document/7393152/},
	doi = {10.1109/FPT.2015.7393152},
	abstract = {Accurate forecasts of future climate with numerical models of atmosphere and ocean are of vital importance. However, forecast quality is often limited by the available computational power. This paper investigates the acceleration of a C-grid shallow water model through the use of reduced precision targeting FPGA technology. Using a double-gyre scenario, we show that the mantissa length of variables can be reduced to 14 bits without affecting the accuracy beyond the error inherent in the model. Our reduced precision FPGA implementation runs 5.4 times faster than a double precision FPGA implementation, and 12 times faster than a multi-threaded CPU implementation. Moreover, our reduced precision FPGA implementation uses 39 times less energy than the CPU implementation and can compute a 100x100 grid for the same energy that the CPU implementation would take for a 29x29 grid.},
	language = {en},
	urldate = {2020-02-25},
	booktitle = {2015 {International} {Conference} on {Field} {Programmable} {Technology} ({FPT})},
	publisher = {IEEE},
	author = {Targett, James Stanley and Niu, Xinyu and Russell, Francis and Luk, Wayne and Jeffress, Stephen and Duben, Peter},
	month = dec,
	year = {2015},
	pages = {208--211},
	file = {Targett et al._2016.pdf:/home/kloewer/papers/Targett et al._2016.pdf:application/pdf}
}

@article{Jeffress2017,
	title = {Bitwise efficiency in chaotic models},
	volume = {473},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0144},
	doi = {10.1098/rspa.2017.0144},
	language = {en},
	number = {2205},
	urldate = {2020-02-25},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Jeffress, Stephen and Düben, Peter and Palmer, Tim},
	month = sep,
	year = {2017},
	pages = {20170144},
	file = {Jeffress, Düben, Palmer_2017.pdf:/home/kloewer/papers/Jeffress, Düben, Palmer_2017.pdf:application/pdf}
}

@article{Gustafson2017,
	title = {Beating {Floating} {Point} at its {Own} {Game}: {Posit} {Arithmetic}},
	volume = {4},
	abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 ﬂoating-point numbers (ﬂoats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like ﬂoats, they round if an answer is inexact. However, they provide compelling advantages over ﬂoats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overﬂow to inﬁnity or underﬂow to zero, and “Nota-Number” (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE ﬂoat FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be signiﬁcantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares ﬂoats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than “approximate computing” methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than ﬂoats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit ﬂoat. In other words, posits beat ﬂoats at their own game.},
	language = {en},
	number = {2},
	journal = {Supercomputing Frontiers and Innovations},
	author = {Gustafson, John L and Yonemoto, Isaac},
	year = {2017},
	pages = {16},
	file = {Gustafson, Yonemoto_2017.pdf:/home/kloewer/papers/Gustafson, Yonemoto_2017.pdf:application/pdf}
}

@article{Vana2017,
	title = {Single {Precision} in {Weather} {Forecasting} {Models}: {An} {Evaluation} with the {IFS}},
	volume = {145},
	issn = {0027-0644, 1520-0493},
	shorttitle = {Single {Precision} in {Weather} {Forecasting} {Models}},
	url = {http://journals.ametsoc.org/doi/10.1175/MWR-D-16-0228.1},
	doi = {10.1175/MWR-D-16-0228.1},
	abstract = {Earth’s climate is a nonlinear dynamical system with scale-dependent Lyapunov exponents. As such, an important theoretical question for modeling weather and climate is how much real information is carried in a model’s physical variables as a function of scale and variable type. Answering this question is of crucial practical importance given that the development of weather and climate models is strongly constrained by available supercomputer power. As a starting point for answering this question, the impact of limiting almost all real-number variables in the forecasting mode of ECMWF Integrated Forecast System (IFS) from 64 to 32 bits is investigated. Results for annual integrations and medium-range ensemble forecasts indicate no noticeable reduction in accuracy, and an average gain in computational efﬁciency by approximately 40\%. This study provides the motivation for more scale-selective reductions in numerical precision.},
	language = {en},
	number = {2},
	urldate = {2020-02-25},
	journal = {Monthly Weather Review},
	author = {Váňa, Filip and Düben, Peter and Lang, Simon and Palmer, Tim and Leutbecher, Martin and Salmond, Deborah and Carver, Glenn},
	month = feb,
	year = {2017},
	pages = {495--502},
	file = {Váňa et al._2017.pdf:/home/kloewer/papers/Váňa et al._2017.pdf:application/pdf}
}

@article{Arbic2008,
	title = {On {Quadratic} {Bottom} {Drag}, {Geostrophic} {Turbulence}, and {Oceanic} {Mesoscale} {Eddies}},
	volume = {38},
	issn = {0022-3670, 1520-0485},
	url = {http://journals.ametsoc.org/doi/10.1175/2007JPO3653.1},
	doi = {10.1175/2007JPO3653.1},
	abstract = {Many investigators have idealized the oceanic mesoscale eddy field with numerical simulations of geostrophic turbulence forced by a horizontally homogeneous, baroclinically unstable mean flow. To date such studies have employed linear bottom Ekman friction (hereinafter, linear drag). This paper presents simulations of two-layer baroclinically unstable geostrophic turbulence damped by quadratic bottom drag, which is generally thought to be more realistic. The goals of the paper are 1) to describe the behavior of quadratically damped turbulence as drag strength changes, using previously reported behaviors of linearly damped turbulence as a point of comparison, and 2) to compare the eddy energies, baroclinicities, and horizontal scales in both quadratic and linear drag simulations with observations and to discuss the constraints these comparisons place on the form and strength of bottom drag in the ocean. In both quadratic and linear drag simulations, large barotropic eddies develop with weak damping, large equivalent barotropic eddies develop with strong damping, and the comparison in goal 2 above is closest when the nondimensional friction strength parameter is of order 1. Typical values of the quadratic drag coefficient (cd ϳ 0.0025) and of boundary layer depths (Hb ϳ 50 m) imply that the quadratic friction strength parameter cdLd /Hb , where Ld is the deformation radius, may indeed be of order 1 in the ocean. Model eddies are realistic over a wider range of friction strengths when drag is quadratic, because of a reduced sensitivity to friction strength in that case. The quadratic parameter is independent of the mean shear, in contrast to the linear parameter. Plots of eddy length scales, computed from satellite altimeter data, versus mean shear and versus rough estimates of the friction strength parameters suggest that both linear and quadratic bottom drag may be active in the ocean. Topographic wave drag contains terms that are linear in the bottom flow, thus providing some justification for the use of linear bottom drag in models.},
	language = {en},
	number = {1},
	urldate = {2020-02-25},
	journal = {Journal of Physical Oceanography},
	author = {Arbic, Brian K. and Scott, Robert B.},
	month = jan,
	year = {2008},
	pages = {84--103},
	file = {Arbic, Scott_2008.pdf:/home/kloewer/papers/Arbic, Scott_2008.pdf:application/pdf}
}

@article{Palmer2014,
	title = {Stochastic modelling and energy-efficient computing for weather and climate prediction},
	volume = {372},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2014.0118},
	doi = {10.1098/rsta.2014.0118},
	language = {en},
	number = {2018},
	urldate = {2020-02-25},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Palmer, Tim and Düben, Peter and McNamara, Hugh},
	month = jun,
	year = {2014},
	pages = {20140118},
	file = {Palmer, Düben, McNamara_2014.pdf:/home/kloewer/papers/Palmer, Düben, McNamara_2014.pdf:application/pdf}
}

@article{Griffies2000,
	title = {Biharmonic {Friction} with a {Smagorinsky}-{Like} {Viscosity} for {Use} in {Large}-{Scale} {Eddy}-{Permitting} {Ocean} {Models}},
	volume = {128},
	abstract = {This paper discusses a numerical closure, motivated from the ideas of Smagorinsky, for use with a biharmonic operator. The result is a highly scale-selective, state-dependent friction operator for use in eddy-permitting geophysical ﬂuid models. This friction should prove most useful for large-scale ocean models in which there are multiple regimes of geostrophic turbulence. Examples are provided from primitive equation geopotential and isopycnal-coordinate ocean models.},
	language = {en},
	journal = {MONTHLY WEATHER REVIEW},
	author = {Griffies, Stephen M and Hallberg, Robert W},
	year = {2000},
	pages = {12},
	file = {Griffies, Hallberg_2000.pdf:/home/kloewer/papers/Griffies, Hallberg_2000.pdf:application/pdf}
}

@book{Gill1982,
	address = {San Diego},
	series = {International geophysics series},
	title = {Atmosphere-ocean dynamics},
	isbn = {978-0-12-283520-9 978-0-12-283522-3},
	language = {en},
	number = {30},
	publisher = {Acad. Press},
	author = {Gill, Adrian E.},
	year = {1982},
	note = {OCLC: 249294465},
	file = {Gill_1982.pdf:/home/kloewer/papers/Gill_1982.pdf:application/pdf}
}

@article{Palmer2012,
	title = {Towards the probabilistic {Earth}-system simulator: a vision for the future of climate and weather prediction},
	volume = {138},
	issn = {00359009},
	shorttitle = {Towards the probabilistic {Earth}-system simulator},
	url = {http://doi.wiley.com/10.1002/qj.1923},
	doi = {10.1002/qj.1923},
	language = {en},
	number = {665},
	urldate = {2020-02-25},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Palmer, T. N.},
	month = apr,
	year = {2012},
	pages = {841--861},
	file = {Palmer_2012(2).pdf:/home/kloewer/papers/Palmer_2012(2).pdf:application/pdf}
}

@article{Morris1971,
	title = {Tapered {Floating} {Point}: {A} {New} {Floating}-{Point} {Representation}},
	volume = {C-20},
	issn = {2326-3814},
	shorttitle = {Tapered {Floating} {Point}},
	doi = {10.1109/T-C.1971.223174},
	abstract = {It is well known that there is a possible tradeoff in the binary representation of floating-point numbers in which one bit of accuracy can be gained at the cost of halving the exponent range, and vice versa. A way in which the exponent range can be greatly increased while preserving full accuracy for most computations is suggested.},
	number = {12},
	journal = {IEEE Transactions on Computers},
	author = {Morris, R.},
	month = dec,
	year = {1971},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Acuracy, exponent range, floating point, number representation.},
	pages = {1578--1579},
	file = {IEEE Xplore Abstract Record:/home/kloewer/papers/storage/ILM24D4E/1671767.html:text/html}
}

@article{Bezanson2017,
	title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
	volume = {59},
	issn = {0036-1445},
	shorttitle = {Julia},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item  High-level dynamic programs have to be slow. {\textbackslash}item  One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
	number = {1},
	urldate = {2020-02-25},
	journal = {SIAM Review},
	author = {Bezanson, Jeff. and Edelman, Alan. and Karpinski, Stefan. and Shah, Viral B.},
	month = jan,
	year = {2017},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {65--98},
	file = {Bezanson et al._2014.pdf:/home/kloewer/papers/Bezanson et al._2014.pdf:application/pdf;Full Text PDF:/home/kloewer/papers/storage/SZM3ENMR/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf;Snapshot:/home/kloewer/papers/storage/UG5NFZ28/141000671.html:text/html}
}

@book{Butcher2016,
	address = {Chichester, West Sussex, United Kingdom},
	edition = {3 edition},
	title = {Numerical {Methods} for {Ordinary} {Differential} {Equations}},
	isbn = {978-1-119-12150-3},
	abstract = {A new edition of this classic work, comprehensively revised to present exciting new developments in this important subject The study of numerical methods for solving ordinary differential equations is constantly developing and regenerating, and this third edition of a popular classic volume, written by one of the world’s leading experts in the field, presents an account of the subject which reflects both its historical and well-established place in computational science and its vital role as a cornerstone of modern applied mathematics. In addition to serving as a broad and comprehensive study of numerical methods for initial value problems, this book contains a special emphasis on Runge-Kutta methods by the mathematician who transformed the subject into its modern form dating from his classic 1963 and 1972 papers.  A second feature is general linear methods which have now matured and grown from being a framework for a unified theory of a wide range of diverse numerical schemes to a source of new and practical algorithms in their own right.  As the founder of general linear method research, John Butcher has been a leading contributor to its development; his special role is reflected in the text.  The book is written in the lucid style characteristic of the author, and combines enlightening explanations with rigorous and precise analysis. In addition to these anticipated features, the book breaks new ground by including the latest results on the highly efficient G-symplectic methods which compete strongly with the well-known symplectic Runge-Kutta methods for long-term integration of conservative mechanical systems. This third edition of Numerical Methods for Ordinary Differential Equations will serve as a key text for senior undergraduate and graduate courses in numerical analysis, and is an essential resource for research workers in applied mathematics, physics and engineering.},
	language = {English},
	publisher = {Wiley},
	author = {Butcher, J. C.},
	month = aug,
	year = {2016}
}

@article{Arakawa1990,
	title = {Energy {Conserving} and {Potential}-{Enstrophy} {Dissipating} {Schemes} for the {Shallow} {Water} {Equations}},
	volume = {118},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/10.1175/1520-0493%281990%29118%3C1960%3AECAPED%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(1990)118<1960:ECAPED>2.0.CO;2},
	abstract = {To incorporate potential enstrophy dissipation into discrete shallow water equations with no or arbitrarily small energy dissipation, a family of finite-difference schemes have been derived with which potential enstrophy is guaranteed to decrease while energy is conserved (when the mass flux is nondivergent and time is continuous). Among this family of schemes, there is a member that minimizes the spurious impact of infinite potential vorticities associated with infinitesimal fluid depth. The scheme is, therefore, useful for problems in which the free surface may intersect with the lower boundary.},
	number = {10},
	urldate = {2020-02-25},
	journal = {Monthly Weather Review},
	author = {Arakawa, Akio and Hsu, Yueh-Jiuan G.},
	month = oct,
	year = {1990},
	note = {Publisher: American Meteorological Society},
	pages = {1960--1969},
	file = {Snapshot:/home/kloewer/papers/storage/8VBFKY27/1520-0493(1990)1181960ECAPED2.0.html:text/html;Full Text PDF:/home/kloewer/papers/storage/UWI4RRFV/Arakawa and Hsu - 1990 - Energy Conserving and Potential-Enstrophy Dissipat.pdf:application/pdf}
}

@incollection{Arakawa1977,
	series = {General {Circulation} {Models} of the {Atmosphere}},
	title = {Computational {Design} of the {Basic} {Dynamical} {Processes} of the {UCLA} {General} {Circulation} {Model}},
	volume = {17},
	url = {http://www.sciencedirect.com/science/article/pii/B9780124608177500094},
	language = {en},
	urldate = {2020-02-25},
	booktitle = {Methods in {Computational} {Physics}: {Advances} in {Research} and {Applications}},
	publisher = {Elsevier},
	author = {Arakawa, AKIO and Lamb, VIVIAN R.},
	editor = {Chang, JULIUS},
	month = jan,
	year = {1977},
	doi = {10.1016/B978-0-12-460817-7.50009-4},
	pages = {173--265},
	file = {ScienceDirect Snapshot:/home/kloewer/papers/storage/CER48SV4/B9780124608177500094.html:text/html}
}

@article{Smolarkiewicz1992,
	title = {A {Class} of {Semi}-{Lagrangian} {Approximations} for {Fluids}},
	volume = {49},
	issn = {0022-4928},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0469%281992%29049%3C2082%3AACOSLA%3E2.0.CO%3B2},
	doi = {10.1175/1520-0469(1992)049<2082:ACOSLA>2.0.CO;2},
	abstract = {This paper discusses a class of finite-difference approximations to the evolution equations of fluid dynamics. These approximations derive from elementary properties of differential forms. Values of a fluid variable ψ at any two points of a space-time continuum are related through the integral of the space-time gradient of ψ along an arbitrary contour connecting these two points (Stokes' theorem). Noting that spatial and temporal components of the gradient are related through the fluid equations, and selecting the contour composed of a parcel trajectory and an appropriate residual, leads to the integral form of the fluid equations, which is particularly convenient for finite-difference approximations. In these equations, the inertial and forcing terms are separated such that forces are integrated along a parcel trajectory (the Lagrangian aspect), whereas advection of the variable is evaluated along the residual contour (the Eulerian aspect). The virtue of this method is an extreme simplicity of the resulting solver; the entire model for a fluid may be essentially built upon a single one-dimensional Eulerian advection scheme while retaining the formal accuracy of its constant-coefficient limit. The Lagrangian aspect of the approach allows for large-Courant-number ({\textgreater}1) computations in a broad spectrum of dynamic applications. Theoretical considerations are illustrated with examples of applications to selected classical problems of atmospheric fluid dynamics. Since the theoretical arguments adopted in this paper assume differentiability of fluid variables, fluid systems admitting truly discontinuous solutions (e.g., shock waves, hydraulic jumps) are formally excluded from our considerations.},
	number = {22},
	urldate = {2020-02-26},
	journal = {Journal of the Atmospheric Sciences},
	author = {Smolarkiewicz, Piotr K. and Pudykiewicz, Janusz A.},
	month = nov,
	year = {1992},
	note = {Publisher: American Meteorological Society},
	pages = {2082--2096},
	file = {Full Text PDF:/home/kloewer/papers/storage/3ZKFM5ZF/Smolarkiewicz and Pudykiewicz - 1992 - A Class of Semi-Lagrangian Approximations for Flui.pdf:application/pdf;Snapshot:/home/kloewer/papers/storage/DH4Y6H5B/1520-0469(1992)0492082ACOSLA2.0.html:text/html}
}

@book{Vallis2006,
	title = {Atmospheric and {Oceanic} {Fluid} {Dynamics}},
	publisher = {Cambridge University Press},
	author = {Vallis, Geoffrey K.},
	year = {2006}
}

@article{Lorenz1963,
	title = {Deterministic {Nonperiodic} {Flow}},
	volume = {20},
	issn = {0022-4928},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2},
	doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
	abstract = {Finite systems of deterministic ordinary nonlinear differential equations may be designed to represent forced dissipative hydrodynamic flow. Solutions of these equations can be identified with trajectories in phase space. For those systems with bounded solutions, it is found that nonperiodic solutions are ordinarily unstable with respect to small modifications, so that slightly differing initial states can evolve into considerably different states. Systems with bounded solutions are shown to possess bounded numerical solutions. A simple system representing cellular convection is solved numerically. All of the solutions are found to be unstable, and almost all of them are nonperiodic. The feasibility of very-long-range weather prediction is examined in the light of these results.},
	number = {2},
	urldate = {2020-02-26},
	journal = {Journal of the Atmospheric Sciences},
	author = {Lorenz, Edward N.},
	month = mar,
	year = {1963},
	note = {Publisher: American Meteorological Society},
	pages = {130--141},
	file = {Full Text PDF:/home/kloewer/papers/storage/LISF2756/Lorenz - 1963 - Deterministic Nonperiodic Flow.pdf:application/pdf;Snapshot:/home/kloewer/papers/storage/RNFRXD2T/1520-0469(1963)0200130DNF2.0.html:text/html}
}

@misc{Gustafson2017a,
	title = {Posit {Arithmetic}},
	url = {https://posithub.org/docs/Posits4.pdf},
	language = {English},
	urldate = {2020-02-26},
	author = {Gustafson, John},
	year = {2017},
	file = {Gustafson_2017.pdf:/home/kloewer/papers/Gustafson_2017.pdf:application/pdf}
}

@article{Palmer2015,
	title = {Modelling: {Build} imprecise supercomputers},
	volume = {526},
	shorttitle = {Modelling},
	url = {http://www.nature.com/news/modelling-build-imprecise-supercomputers-1.18437},
	doi = {10.1038/526032a},
	abstract = {Energy-optimized hybrid computers with a range of processor accuracies will advance modelling in fields from climate change to neuroscience, says Tim Palmer.},
	language = {en},
	number = {7571},
	urldate = {2020-02-26},
	journal = {Nature News},
	author = {Palmer, Tim},
	month = oct,
	year = {2015},
	note = {Section: Comment},
	pages = {32},
	file = {Snapshot:/home/kloewer/papers/storage/QL9D2WFY/modelling-build-imprecise-supercomputers-1.html:text/html}
}

@inproceedings{vanDam2019,
	address = {Singapore, Singapore},
	series = {{CoNGA}'19},
	title = {An {Accelerator} for {Posit} {Arithmetic} {Targeting} {Posit} {Level} 1 {BLAS} {Routines} and {Pair}-{HMM}},
	isbn = {978-1-4503-7139-1},
	url = {https://doi.org/10.1145/3316279.3316284},
	doi = {10.1145/3316279.3316284},
	abstract = {The newly proposed posit number format uses a significantly different approach to represent floating point numbers. This paper introduces a framework for posit arithmetic in reconfigurable logic that maintains full precision in intermediate results. We present the design and implementation of a L1 BLAS arithmetic accelerator on posit vectors leveraging Apache Arrow. For a vector dot product with an input vector length of 106 elements, a hardware speedup of approximately 104 is achieved as compared to posit software emulation. For 32-bit numbers, the decimal accuracy of the posit dot product results improve by one decimal of accuracy on average compared to a software implementation, and two extra decimals compared to the IEEE754 format. We also present a posit-based implementation of pair-HMM. In this case, the hardware speedup vs. a posit-based software implementation ranges from 105 to 106. With appropriate initial scaling constants, accuracy improves on an implementation based on IEEE 754.},
	urldate = {2020-02-26},
	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic} 2019},
	publisher = {Association for Computing Machinery},
	author = {van Dam, Laurens and Peltenburg, Johan and Al-Ars, Zaid and Hofstee, H. Peter},
	month = mar,
	year = {2019},
	keywords = {accelerator, arithmetic, BLAS, decimal accuracy, FPGA, pair-HMM, posit, unum, unum-III},
	pages = {1--10},
	file = {Full Text PDF:/home/kloewer/papers/storage/RR2FCBD4/van Dam et al. - 2019 - An Accelerator for Posit Arithmetic Targeting Posi.pdf:application/pdf}
}

@article{Rudisuhli2013,
	title = {{COSMO} in single precision},
	number = {14},
	journal = {Cosmo Newsletter},
	author = {Rüdisühli, Stefan and Walser, André and Fuhrer, Oliver},
	year = {2013},
	pages = {5--1},
	file = {Full Text:/home/kloewer/papers/storage/ZJHTAGB2/Rüdisühli et al. - 2013 - COSMO in single precision.pdf:application/pdf}
}

@misc{Klower2019a,
	title = {{SoftPosit}.jl -  {A} posit arithmetic emulator},
	shorttitle = {milankl/{SoftPosit}.jl},
	url = {https://zenodo.org/record/3590291},
	urldate = {2020-02-26},
	publisher = {Zenodo},
	author = {Klöwer, Milan and Giordano, Mose},
	month = dec,
	year = {2019},
	doi = {10.5281/zenodo.3590291},
	file = {Zenodo Snapshot:/home/kloewer/papers/storage/U7UMAYDJ/3590291.html:text/html}
}

@article{Jouppi2018,
	title = {Motivation for and {Evaluation} of the {First} {Tensor} {Processing} {Unit}},
	volume = {38},
	issn = {1937-4143},
	doi = {10.1109/MM.2018.032271057},
	abstract = {The first-generation tensor processing unit (TPU) runs deep neural network (DNN) inference 15-30 times faster with 30-80 times better energy efficiency than contemporary CPUs and GPUs in similar semiconductor technologies. This domain-specific architecture (DSA) is a custom chip that has been deployed in Google datacenters since 2015, where it serves billions of people.},
	number = {3},
	journal = {IEEE Micro},
	author = {Jouppi, Norman and Young, Cliff and Patil, Nishant and Patterson, David},
	month = may,
	year = {2018},
	note = {Conference Name: IEEE Micro},
	keywords = {Central Processing Unit, computer centres, coprocessors, Data centers, deep neural network, deep neural network inference, DNN, domain-specific architecture, energy efficiency, Energy efficiency, feedforward neural nets, first-generation tensor processing unit, Google datacenters, GPU, Graphics processing units, hardware, machine learning, microprocessor, Microprocessors, motivation, Neural networks, parallel architectures, Semiconductor devices, semiconductor technologies, Tensile stress, tensor processing unit, tensors, TPU},
	pages = {10--19},
	file = {IEEE Xplore Abstract Record:/home/kloewer/papers/storage/YVFJ4I7D/8358031.html:text/html}
}

@inproceedings{Jouppi2017,
	address = {Toronto, ON, Canada},
	series = {{ISCA} '17},
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}},
	isbn = {978-1-4503-4892-8},
	url = {https://doi.org/10.1145/3079856.3080246},
	doi = {10.1145/3079856.3080246},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
	urldate = {2020-02-26},
	booktitle = {Proceedings of the 44th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	month = jun,
	year = {2017},
	keywords = {accelerator, CNN, deep learning, DNN, domain-specific architecture, GPU, LSTM, MLP, neural network, RNN, TensorFlow, TPU},
	pages = {1--12},
	file = {Full Text PDF:/home/kloewer/papers/storage/V7Z7TZAM/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf:application/pdf}
}

@article{Jouppi2018a,
	title = {A domain-specific architecture for deep neural networks},
	volume = {61},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3154484},
	doi = {10.1145/3154484},
	abstract = {Tensor processing units improve performance per watt of neural networks in Google datacenters by roughly 50x.},
	number = {9},
	urldate = {2020-02-26},
	journal = {Communications of the ACM},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David},
	month = aug,
	year = {2018},
	pages = {50--59},
	file = {Full Text PDF:/home/kloewer/papers/storage/Z4NRNJF9/Jouppi et al. - 2018 - A domain-specific architecture for deep neural net.pdf:application/pdf}
}

@inproceedings{Burgess2019,
	title = {Bfloat16 {Processing} for {Neural} {Networks}},
	doi = {10.1109/ARITH.2019.00022},
	abstract = {Bfloat16 ("BF16") is a new floating-point format tailored specifically for high-performance processing of Neural Networks and will be supported by major CPU and GPU architectures as well as Neural Network accelerators. This paper proposes a possible implementation of a BF16 multiply-accumulation operation that relaxes several IEEE Floating-Point Standard features to afford low-cost hardware implementations. Specifically, subnorms are flushed to zero; only one non-standard rounding mode (Round-Odd) is supported; NaNs are not propagated; and IEEE exception flags are not provided. The paper shows that this approach achieves the same network-level accuracy as using IEEE single-precision arithmetic ("FP32") for less than half the datapath area cost and with greater throughput.},
	booktitle = {2019 {IEEE} 26th {Symposium} on {Computer} {Arithmetic} ({ARITH})},
	author = {Burgess, Neil and Milanovic, Jelena and Stephens, Nigel and Monachopoulos, Konstantinos and Mansell, David},
	month = jun,
	year = {2019},
	note = {ISSN: 1063-6889},
	keywords = {Artificial neural networks, BF16 multiply-accumulation operation, bfloat16 processing, Computer architecture, CPU architectures, Digital arithmetic, Error analysis, floating point arithmetic, floating-point, rounding mode, neural networks, FP32, GPU architectures, high-performance processing, IEEE exception flags, IEEE floating-point standard features, IEEE single-precision arithmetic, low-cost hardware implementations, network-level accuracy, neural nets, Neural Network accelerators, neural networks, nonstandard rounding mode, Standards, Training},
	pages = {88--91},
	file = {IEEE Xplore Full Text PDF:/home/kloewer/papers/storage/XMKM6HAG/Burgess et al. - 2019 - Bfloat16 Processing for Neural Networks.pdf:application/pdf;IEEE Xplore Abstract Record:/home/kloewer/papers/storage/VC5Y2QAC/8877390.html:text/html}
}

@article{Kalamkar2019,
	title = {A {Study} of {BFLOAT16} for {Deep} {Learning} {Training}},
	url = {http://arxiv.org/abs/1905.12322},
	abstract = {This paper presents the first comprehensive empirical study demonstrating the efficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classification, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 floating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision floating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the flow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorflow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.},
	urldate = {2020-02-26},
	journal = {arXiv:1905.12322 [cs, stat]},
	author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.12322},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kloewer/papers/storage/IBGRBTIN/Kalamkar et al. - 2019 - A Study of BFLOAT16 for Deep Learning Training.pdf:application/pdf;arXiv.org Snapshot:/home/kloewer/papers/storage/V4DDGS7C/1905.html:text/html}
}

@inproceedings{Markidis2018,
	title = {{NVIDIA} {Tensor} {Core} {Programmability}, {Performance} {Precision}},
	doi = {10.1109/IPDPSW.2018.00091},
	abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
	month = may,
	year = {2018},
	note = {ISSN: null},
	keywords = {application program interfaces, Computer architecture, cuBLAS GEMM, CUDA Warp Matrix Multiply Accumulate API, CUDA WMMA API, CUTLASS, GEMM, GPU Programming, graphics processing units, Graphics processing units, Hardware, HPC applications, Instruction sets, matrix multiplication, Mixed Precision, Neural networks, NVIDIA Tensor Core programmability, NVIDIA Tensor Cores, NVIDIA Tesla V100 accelerator, NVIDIA Volta GPU microarchitecture, parallel architectures, parallel programming, Programming, programming matrix-multiply- accumulate, Tensile stress, tensors, Tesla V100 GPU, Volta microarchitecture},
	pages = {522--531},
	file = {IEEE Xplore Full Text PDF:/home/kloewer/papers/storage/9GJDHGPW/Markidis et al. - 2018 - NVIDIA Tensor Core Programmability, Performance Pr.pdf:application/pdf;IEEE Xplore Abstract Record:/home/kloewer/papers/storage/D2WE2S8V/8425458.html:text/html}
}

@article{Langroudi2019,
	title = {Deep {Learning} {Training} on the {Edge} with {Low}-{Precision} {Posits}},
	url = {http://arxiv.org/abs/1907.13216},
	abstract = {Recently, the posit numerical format has shown promise for DNN data representation and compute with ultra-low precision ([5..8]-bit). However, majority of studies focus only on DNN inference. In this work, we propose DNN training using posits and compare with the floating point training. We evaluate on both MNIST and Fashion MNIST corpuses, where 16-bit posits outperform 16-bit floating point for end-to-end DNN training.},
	urldate = {2020-02-26},
	journal = {arXiv:1907.13216 [cs, stat]},
	author = {Langroudi, Hamed F. and Carmichael, Zachariah and Kudithipudi, Dhireesha},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.13216},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kloewer/papers/storage/FJE5BC8K/Langroudi et al. - 2019 - Deep Learning Training on the Edge with Low-Precis.pdf:application/pdf;arXiv.org Snapshot:/home/kloewer/papers/storage/2ST243AB/1907.html:text/html}
}