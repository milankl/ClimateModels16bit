@article{Palmer2012a,
abstract = {There is no more challenging problem in computational science than that of estimating, as accurately as science and technology allows, the future evolution of Earth's climate; nor indeed is there a problem whose solution has such importance and urgency. Historically, the simulation tools needed to predict climate have been developed, somewhat independently, at a number of weather and climate institutes around the world. While these simulators are individually deterministic, it is often assumed that the resulting diversity provides a useful quantification of uncertainty in global or regional predictions. However, this notion is not well founded theoretically and corresponding ‘multi-simulator' estimates of uncertainty can be prone to systemic failure. Separate to this, individual institutes are now facing considerable challenges in finding the human and computational resources needed to develop more accurate weather and climate simulators with higher resolution and full Earth-system complexity. A new approach, originally designed to improve reliability in ensemble-based numerical weather prediction, is introduced to help solve these two rather different problems. Using stochastic mathematics, this approach recognizes uncertainty explicitly in the parametrized representation of unresolved climatic processes. Stochastic parametrization is shown to be more consistent with the underlying equations of motion and, moreover, provides more skilful estimates of uncertainty when compared with estimates from traditional multi-simulator ensembles, on time-scales where verification data exist. Stochastic parametrization can also help reduce long-term biases which have bedevilled numerical simulations of climate from the earliest days to the present. As a result, it is suggested that the need to maintain a large ‘gene pool' of quasi-independent deterministic simulators may be obviated by the development of probabilistic Earth-system simulators. Consistent with the conclusions of the World Summit on Climate Modelling, this in turn implies that individual institutes will be able to pool human and computational resources in developing future-generation simulators, thus benefitting from economies of scale; the establishment of the Airbus consortium provides a useful analogy here. As a further stimulus for such evolution, discussion is given to a potential new synergy between the development of dynamical cores, and stochastic processing hardware. However, it is concluded that the traditional challenge in numerical weather prediction, of reducing deterministic measures of forecast error, may increasingly become an obstacle to the seamless development of probabilistic weather and climate simulators, paradoxical as that may appear at first sight. Indeed, going further, it is argued that it may be time to consider focusing operational weather forecast development entirely on high-resolution ensemble prediction systems. Finally, by considering the exceptionally challenging problem of quantifying cloud feedback in climate change, it is argued that the development of the probabilistic Earth-system simulator may actually provide a route to reducing uncertainty in climate prediction. Copyright {\textcopyright} 2012 Royal Meteorological Society},
author = {Palmer, T. N.},
doi = {10.1002/qj.1923},
file = {:home/kloewer/papers/Palmer{\_}2012(2).pdf:pdf},
isbn = {1477-870X},
issn = {00359009},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {Earth-system simulation,Ensemble prediction,Stochastic parametrization},
number = {665},
pages = {841--861},
title = {{Towards the probabilistic Earth-system simulator: A vision for the future of climate and weather prediction}},
volume = {138},
year = {2012}
}
@article{Arakawa1977,
author = {Arakawa, Akio and Lamb, V R},
journal = {Methods of Computational Physics},
pages = {173--265},
title = {{Computational design of the basic dynamical processes of the UCLA general circulation model}},
volume = {17},
year = {1977}
}
@article{Palmer2001,
abstract = {Conventional parametrization schemes in weather and climate prediction models describe the effects of subgrid-scale processes by deterministic bulk formulae which depend on local resolved-scale variables and a number of adjustable parameters. Despite the unquestionable success of such models for weather and climate prediction, it is impossible to justify the use of such formulae from first principles. Using low-order dynamical- systems models, and elementary results from dynamical-systems and turbulence theory, it is shown that even if unresolved scales only describe a small fraction of the total variance of the system, neglecting their variability can, in some circumstances, lead to gross errors in the climatology of the dominant scales. It is suggested that some of the remaining errors in weather and climate prediction models may have their origin in the neglect of subgrid-scale variability, and that such variability should be parametrized by non-local dynamically based stochastic parametrization schemes. Results from existing schemes are described, and mechanisms which might account for the impact of random parametrization error on planetary-scale motions are discussed. Proposals for the development of non-local stochastic-dynamic parametrization schemes are outlined, based on potential-vorticity diagnosis, singular-vector analysis and a simple stochastic cellular automaton model.},
author = {Palmer, T. N.},
doi = {10.1002/qj.49712757202},
file = {:home/kloewer/papers/Palmer{\_}2001.pdf:pdf},
isbn = {1477-870X},
issn = {00359009},
journal = {Quarterly Journal of the Royal Meteorological Society},
pages = {279--304},
pmid = {4572442},
title = {{A nonlinear dynamical perspective on model error: A proposal for non-local stochastic-dynamic parametrization in weather and climate prediction models}},
volume = {127},
year = {2001}
}
@article{Rudisuhli2013,
author = {R{\"{u}}dis{\"{u}}hli, Stefan and Walser, Andr{\'{e}} and Fuhrer, Oliver},
file = {:home/kloewer/papers/R{\"{u}}dis{\"{u}}hli, Walser, Fuhrer{\_}2013.pdf:pdf},
journal = {COSMO Newsletter},
title = {{COSMO in single precision}},
url = {http://www.cosmo-model.org/content/model/documentation/newsLetters/newsLetter14/cnl14_09.pdf},
year = {2013}
}
@article{Chen2018,
author = {Chen, Jianyu and Hofstee, H Peter},
doi = {10.1145/3190339.3190340},
file = {:home/kloewer/papers/Chen, Hofstee{\_}2018.pdf:pdf},
isbn = {9781450364140},
keywords = {2018,Posit number, Matrix-Multiplier, Dot-product,a matrix-multiply,acm reference format,and h,dot-product,jianyu chen,matrix-multiplier,peter hofstee,posit number,zaid al-ars},
pages = {1--5},
title = {{A Matrix-Multiply Unit for Posits in Reconfigurable Logic Leveraging ( Open ) CAPI}},
year = {2018}
}
@book{Holton2004,
author = {Holton, James R},
edition = {4},
file = {:home/kloewer/papers/Holton{\_}2004.pdf:pdf},
pages = {553},
publisher = {Academic Press},
title = {{An Introduction to Dynamic Meteorology}},
year = {2004}
}
@article{IEEE,
doi = {10.1109/IEEESTD.2008.4610935},
author = {IEEE},
journal = {IEEE Std 754-2008},
keywords = {754-2008,IEEE standard,IEEE standards,NaN,arithmetic,arithmetic formats,binary,computer,computer programming,decimal,decimal floating-point arithmetic,exponent,floating point arithmetic,floating-point,format,interchange,number,programming,rounding,significand,subnormal},
month = {aug},
pages = {1--70},
title = {{IEEE Standard for Floating-Point Arithmetic}},
year = {2008}
}
@article{Smolarkiewicz1992,
abstract = {Abstract This paper discusses a class of finite-difference approximations to the evolution equations of fluid dynamics. These approximations derive from elementary properties of differential forms. Values of a fluid variable $\psi$ at any two points of a space-time continuum are related through the integral of the space-time gradient of $\psi$ along an arbitrary contour connecting these two points (Stokes' theorem). Noting that spatial and temporal components of the gradient are related through the fluid equations, and selecting the contour composed of a parcel trajectory and an appropriate residual, leads to the integral form of the fluid equations, which is particularly convenient for finite-difference approximations. In these equations, the inertial and forcing terms are separated such that forces are integrated along a parcel trajectory (the Lagrangian aspect), whereas advection of the variable is evaluated along the residual contour (the Eulerian aspect). The virtue of this method is an extreme simplicity of t...},
author = {Smolarkiewicz, Piotr K. and Pudykiewicz, Janusz A.},
doi = {10.1175/1520-0469(1992)049<2082:ACOSLA>2.0.CO;2},
file = {:home/kloewer/papers/Smolarkiewicz, Pudykiewicz{\_}1992.pdf:pdf},
isbn = {0022-4928},
issn = {0022-4928},
journal = {Journal of the atmospheric sciences},
number = {22},
pages = {2082--2096},
title = {{A Class of Semi-Lagrangian Approximations for Fluids}},
volume = {49},
year = {1992}
}
@article{Grassberger1983,
abstract = {We study the correlation exponent v introduced recently as a characteristic measure of strange attractors which allows one to distinguish between deterministic chaos and random noise. The exponent v is closely related to the fractal dimension and the information dimension, but its computation is considerably easier. Its usefulness in characterizing experimental data which stem from very high dimensional systems is stressed. Algorithms for extracting v from the time series of a single variable are proposed. The relations between the various measures of strange attractors and between them and the Lyapunov exponents are discussed. It is shown that the conjecture of Kaplan and Yorke for the dimension gives an upper bound for v. Various examples of finite and infinite dimensional systems are treated, both numerically and analytically. {\textcopyright} 1983.},
author = {Grassberger, Peter and Procaccia, Itamar},
doi = {10.1016/0167-2789(83)90298-1},
file = {:home/kloewer/papers/Grassberger, Procaccia{\_}1983.pdf:pdf},
isbn = {978-981-02-2310-6},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-2},
pages = {189--208},
title = {{Measuring the strangeness of strange attractors}},
volume = {9},
year = {1983}
}
@article{Griffies2000,
abstract = {Abstract This paper discusses a numerical closure, motivated from the ideas of Smagorinsky, for use with a biharmonic operator. The result is a highly scale-selective, state-dependent friction operator for use in eddy-permitting geophysical fluid models. This friction should prove most useful for large-scale ocean models in which there are multiple regimes of geostrophic turbulence. Examples are provided from primitive equation geopotential and isopycnal-coordinate ocean models.},
author = {Griffies, Stephen M and Hallberg, Robert},
doi = {10.1175/1520-0493(2000)128<2935},
file = {:home/kloewer/papers/Griffies, Hallberg{\_}2000.pdf:pdf},
isbn = {0027-0644},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {8},
pages = {2935--2946},
title = {{Biharmonic Friction with a Smagorinsky-Like Viscosity for Use in Large-Scale Eddy-Permitting Ocean Models}},
volume = {128},
year = {2000}
}
@article{Griffies2000a,
abstract = {This paper presents some research developments in primitive equation ocean models which could impact the ocean component of realistic global coupled climate models aimed at large-scale, low frequency climate simulations and predictions. It is written primarily to an audience of modellers concerned with the ocean component of climate models, although not necessarily experts in the design and implementation of ocean model algorithms.},
author = {Griffies, Stephen M. and B{\"{o}}ning, Claus and Bryan, Frank O. and Chassignet, Eric P. and Gerdes, R{\"{u}}diger and Hasumi, Hiroyasu and Hirst, Anthony and Treguier, Anne-Marie and Webb, David},
doi = {10.1016/S1463-5003(00)00014-7},
file = {:home/kloewer/papers/Griffies et al.{\_}2000.pdf:pdf},
isbn = {1463-5003},
issn = {14635003},
journal = {Ocean Modelling},
number = {3-4},
pages = {123--192},
pmid = {7921454782765478477},
title = {{Developments in ocean climate modelling}},
volume = {2},
year = {2000}
}
@book{Vallis2006,
author = {Vallis, G K},
pages = {773},
publisher = {Cambridge University Press},
title = {{Atmospheric and Ocean Fluid Dynamics}},
year = {2006}
}
@book{Gill1982,
author = {Gill, A E},
file = {:home/kloewer/papers/Gill{\_}1982.pdf:pdf},
isbn = {0122835204},
pages = {662},
publisher = {Academic Press},
title = {{Atmosphere-Ocean Dynamics}},
year = {1982}
}
@article{Glaser2017,
abstract = {To overcome the limitations of conventional floating-point number formats, an interval arithmetic and variable-width storage format called universal number (unum) has been recently introduced. This paper presents the first (to the best of our knowledge) silicon implementation measurements of an application-specific integrated circuit (ASIC) for unum floating-point arithmetic. The designed chip includes a 128-bit wide unum arithmetic unit to execute additions and subtractions, while also supporting lossless (for intermediate results) and lossy (for external data movements) compression units to exploit the memory usage reduction potential of the unum format. Our chip, fabricated in a 65 nm CMOS process, achieves a maximum clock frequency of 413 MHz at 1.2 V with an average measured power of 210 uW/MHz.},
archivePrefix = {arXiv},
arxivId = {1712.01021},
author = {Glaser, Florian and Mach, Stefan and Rahimi, Abbas and G{\"{u}}rkaynak, Frank K. and Huang, Qiuting and Benini, Luca},
doi = {10.1109/ISCAS.2018.8351546},
eprint = {1712.01021},
file = {:home/kloewer/papers/Glaser et al.{\_}2017.pdf:pdf},
isbn = {2379-447X VO -},
title = {{An 826 MOPS, 210 uW/MHz Unum ALU in 65 nm}},
url = {http://arxiv.org/abs/1712.01021},
year = {2017}
}
@misc{Arakawa1990,
abstract = {To incorporate potential enstrophy dissipation into discrete shallow water equations with no or arbitrarily small energy dissipation, a family of finite-difference schemes have been derived with which potential enstrophy is guaranteed to decrease while energy is conserved (when the mass flux is nondivergent and time is continuous). Among this family of schemes, there is a member that minimizes the spurious impact of infinite potential vorticities associated with infinitesimal fluid depth. The scheme is, therefore, useful for problems in which the free surface may intersect with the lower boundary.},
author = {Arakawa, Akio and Hsu, Yueh-Jiuan G},
booktitle = {Monthly Weather Review},
doi = {10.1175/1520-0493(1990)118<1960:ECAPED>2.0.CO;2},
file = {:home/kloewer/papers/Arakawa, Hsu{\_}1990.pdf:pdf},
isbn = {0027-0644},
issn = {0027-0644},
number = {10},
pages = {1960--1969},
title = {{Energy Conserving and Potential-Enstrophy Dissipating Schemes for the Shallow Water Equations}},
url = {http://dx.doi.org/10.1175/1520-0493(1990)118{\%}3C1960:ECAPED{\%}3E2.0.CO{\%}5Cn2},
volume = {118},
year = {1990}
}
@article{Arbic2008,
abstract = {Abstract Many investigators have idealized the oceanic mesoscale eddy field with numerical simulations of geostrophic turbulence forced by a horizontally homogeneous, baroclinically unstable mean flow. To date such studies have employed linear bottom Ekman friction (hereinafter, linear drag). This paper presents simulations of two-layer baroclinically unstable geostrophic turbulence damped by quadratic bottom drag, which is generally thought to be more realistic. The goals of the paper are 1) to describe the behavior of quadratically damped turbulence as drag strength changes, using previously reported behaviors of linearly damped turbulence as a point of comparison, and 2) to compare the eddy energies, baroclinicities, and horizontal scales in both quadratic and linear drag simulations with observations and to discuss the constraints these comparisons place on the form and strength of bottom drag in the ocean. In both quadratic and linear drag simulations, large barotropic eddies develop with weak damping, large equivalent barotropic eddies develop with strong damping, and the comparison in goal 2 above is closest when the nondimensional friction strength parameter is of order 1. Typical values of the quadratic drag coefficient (cd ? 0.0025) and of boundary layer depths (Hb ? 50 m) imply that the quadratic friction strength parameter cdLd/Hb, where Ld is the deformation radius, may indeed be of order 1 in the ocean. Model eddies are realistic over a wider range of friction strengths when drag is quadratic, because of a reduced sensitivity to friction strength in that case. The quadratic parameter is independent of the mean shear, in contrast to the linear parameter. Plots of eddy length scales, computed from satellite altimeter data, versus mean shear and versus rough estimates of the friction strength parameters suggest that both linear and quadratic bottom drag may be active in the ocean. Topographic wave drag contains terms that are linear in the bottom flow, thus providing some justification for the use of linear bottom drag in models.},
author = {Arbic, Brian K. and Scott, Robert B.},
doi = {10.1175/2007JPO3653.1},
file = {:home/kloewer/papers/Arbic, Scott{\_}2008.pdf:pdf},
isbn = {0022-3670$\backslash$r1520-0485},
issn = {0022-3670},
journal = {Journal of Physical Oceanography},
number = {1},
pages = {84--103},
title = {{On Quadratic Bottom Drag, Geostrophic Turbulence, and Oceanic Mesoscale Eddies}},
volume = {38},
year = {2008}
}
@article{Kwasniok2014,
abstract = {One contribution of 14 to a Theme Issue 'Stochastic modelling and energy-efficient computing for weather and climate prediction' . Regime predictability in atmospheric low-order models augmented with stochastic forcing is studied. Atmospheric regimes are identified as persistent or metastable states using a hidden Markov model analysis. A somewhat counterintuitive, coherence resonance-like effect is observed: regime predictability increases with increasing noise level up to an intermediate optimal value, before decreasing when further increasing the noise level. The enhanced regime predictability is due to increased persistence of the regimes. The effect is found in the Lorenz '63 model and a low-order model of barotropic flow over topography. The increased predictability is only present in the regime dynamics, that is, in a coarse-grained view of the system; predictability of individual trajectories decreases monotonically with increasing noise level. A possible explanation for the phenomenon is given and implications of the finding for weather and climate modelling and prediction are discussed.},
author = {Kwasniok, Frank},
doi = {10.1098/rsta.2013.0286},
file = {:home/kloewer/papers/Kwasniok{\_}2014.pdf:pdf},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Atmospheric regimes,Predictability,Stochastic modelling},
number = {2018},
title = {{Enhanced regime predictability in atmospheric low-order models due to stochastic forcing}},
volume = {372},
year = {2014}
}
@article{McGuinness1983,
abstract = {Taken's box-counting algorithm for computing the fractal dimension of a strange attractor is applied to the Lorenz equation. A convergence problem is discussed, and an approximate dimension is computed. {\textcopyright} 1983.},
author = {McGuinness, Mark J.},
doi = {10.1016/0375-9601(83)90052-X},
file = {:home/kloewer/papers/McGuinness{\_}1983.pdf:pdf},
issn = {03759601},
journal = {Physics Letters A},
number = {1},
pages = {5--9},
title = {{The fractal dimension of the Lorenz attractor}},
volume = {99},
year = {1983}
}
@article{Bezanson2014,
abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast. Julia questions notions generally held as "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow. 2. One must prototype in one language and then rewrite in another language for speed or deployment, and 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. We introduce the Julia programming language and its design --- a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can have machine performance without sacrificing human convenience.},
archivePrefix = {arXiv},
arxivId = {1411.1607},
author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
doi = {10.1137/141000671},
eprint = {1411.1607},
file = {:home/kloewer/papers/Bezanson et al.{\_}2014.pdf:pdf},
issn = {0036-1445},
keywords = {10,1137,141000671,65y05,68n15,97p40,ams subject classifications,doi,julia,numerical,parallel,scientific computing},
number = {1},
pages = {65--98},
title = {{Julia: A Fresh Approach to Numerical Computing}},
url = {http://arxiv.org/abs/1411.1607},
volume = {59},
year = {2014}
}
@phdthesis{VanDam2018,
author = {van Dam, Laurens},
file = {:home/kloewer/papers/van Dam{\_}2018.pdf:pdf},
isbn = {9789461869579},
pages = {112},
school = {Delft University of Technology},
title = {{Enabling High Performance Posit Arithmetic Applications Using Hardware Acceleration}},
year = {2018}
}
@article{Palmer2014a,
author = {Palmer, Tim and D{\"{u}}ben, Peter and McNamara, Hugh},
doi = {10.1098/rsta.2014.0118},
file = {:home/kloewer/papers/Palmer, D{\"{u}}ben, McNamara{\_}2014.pdf:pdf},
isbn = {1364-503X},
issn = {1364-503X},
journal = {Philosophical transactions. Series A},
number = {2018},
pages = {20140118},
pmid = {24842039},
title = {{Stochastic modelling and energy-efficient computing for weather and climate prediction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24842039{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4024240},
volume = {372},
year = {2014}
}
@article{Duben2014a,
abstract = {The use of stochastic processing hardware and low precision arithmetic in atmospheric models is investigated. Stochastic processors allow hardware-induced faults in calculations, sacrificing bit-reproducibility and precision in exchange for improvements in performance and potentially accuracy of forecasts, due to a reduction in power consumption that could allow higher resolution. A similar trade-off is achieved using low precision arithmetic, with improvements in computation and communication speed and savings in storage and memory requirements. As high-performance computing becomes more massively parallel and power intensive, these two approaches may be important stepping stones in the pursuit of global cloud-resolving atmospheric modelling. The impact of both hardware induced faults and low precision arithmetic is tested using the Lorenz '96 model and the dynamical core of a global atmosphere model. In the Lorenz '96 model there is a natural scale separation; the spectral discretisation used in the dynamical core also allows large and small scale dynamics to be treated separately within the code. Such scale separation allows the impact of lower-accuracy arithmetic to be restricted to components close to the truncation scales and hence close to the necessarily inexact parametrised representations of unresolved processes. By contrast, the larger scales are calculated using high precision deterministic arithmetic. Hardware faults from stochastic processors are emulated using a bit-flip model with different fault rates. Our simulations show that both approaches to inexact calculations do not substantially affect the large scale behaviour, provided they are restricted to act only on smaller scales. By contrast, results from the Lorenz '96 simulations are superior when small scales are calculated on an emulated stochastic processor than when those small scales are parametrised. This suggests that inexact calculations at the small scale could reduce computation and power costs without adversely affecting the quality of the simulations. This would allow higher resolution models to be run at the same computational cost. {\textcopyright} 2013 The Authors.},
author = {D{\"{u}}ben, Peter D. and McNamara, Hugh and Palmer, T. N.},
doi = {10.1016/j.jcp.2013.10.042},
file = {:home/kloewer/papers/D{\"{u}}ben, McNamara, Palmer{\_}2014.pdf:pdf},
isbn = {0021-9991},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Atmospheric models,Lorenz '96,Scale separation,Single precision,Spectral discretisation,Stochastic processor},
pages = {2--18},
publisher = {Elsevier Inc.},
title = {{The use of imprecise processing to improve accuracy in weather {\&} climate prediction}},
url = {http://dx.doi.org/10.1016/j.jcp.2013.10.042},
volume = {271},
year = {2014}
}
@article{Palmer2014,
author = {Palmer, Tim},
doi = {10.1038/515338a},
file = {:home/kloewer/papers/Palmer{\_}2014.pdf:pdf},
journal = {Natural Resources Forum},
pages = {338--339},
title = {{Build high-resolution global climate models}},
volume = {515},
year = {2014}
}
@misc{Sadourny1975,
abstract = {Abstract Two simple numerical models of the shallow-water equations identical in all respects but for their con-servation properties have been tested regarding their internal mixing processes. The experiments show that violation of enstrophy conservation results in a spurious accumulation of rotational energy in the smaller scales, reflected by an unrealistic increase of enstrophy, which ultimately produces a finite rate of energy dissipation in the zero viscosity limit, thus violating the well-known dynamics of two-dimensional flow. Further, the experiments show a tendency to equipartition of the kinetic energy of the divergent part of the flow in the inviscid limit, suggesting the possibility of a divergent energy cascade in the physical system, as well as a possible influence of the energy mixing on the process of adjustment toward balanced flow.},
author = {Sadourny, Robert},
booktitle = {Journal of the Atmospheric Sciences},
doi = {10.1175/1520-0469(1975)032<0680:TDOFDM>2.0.CO;2},
file = {:home/kloewer/papers/Sadourny{\_}1975.pdf:pdf},
issn = {0022-4928},
number = {4},
pages = {680--689},
title = {{The Dynamics of Finite-Difference Models of the Shallow-Water Equations}},
url = {http://dx.doi.org/10.1175/1520-0469(1975)032{\%}3C0680:TDOFDM{\%}3E2.0.CO{\%}5Cn2},
volume = {32},
year = {1975}
}
@book{Butcher2008,
author = {Butcher, J C},
edition = {2nd},
file = {:home/kloewer/papers/Butcher{\_}2008.pdf:pdf},
isbn = {9780470723357},
publisher = {Wiley},
title = {{Numerical Methods for Ordinary Differential Equations}},
year = {2008}
}
@article{Palmer2016,
author = {Palmer, T N},
file = {:home/kloewer/papers/Palmer{\_}2016.pdf:pdf},
title = {{A personal perspective on modelling the climate system}},
year = {2016}
}
@article{Lorenz1963,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Lorenz, Edward N},
doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
eprint = {NIHMS150003},
file = {:home/kloewer/papers/Lorenz{\_}1963.pdf:pdf},
isbn = {0022-4928},
issn = {0022-4928},
journal = {Journal of the Atmospheric Sciences},
pages = {130--141},
pmid = {20394051},
title = {{Deterministic Nonperiodic Flow}},
volume = {20},
year = {1963}
}
@article{Jeffress2017,
abstract = {Motivated by the increasing energy consumption of supercomputing for weather and climate simulations, we introduce a framework for investigating the bit-level information efficiency of chaotic models. In comparison with previous explorations of inexactness in climate modelling, the proposed and tested information metric has three specific advantages: (i) it requires only a single high-precision time series; (ii) information does not grow indefinitely for decreasing time step; and (iii) information is more sensitive to the dynamics and uncertainties of the model rather than to the implementation details. We demonstrate the notion of bit-level information efficiency in two of Edward Lorenz's prototypical chaotic models: Lorenz 1963 (L63) and Lorenz 1996 (L96). Although L63 is typically integrated in 64-bit ‘double' floating point precision, we show that only 16 bits have significant information content, given an initial condition uncertainty of approximately 1{\%} of the size of the attractor. This result is sensitive to the size of the uncertainty but not to the time step of the model. We then apply the metric to the L96 model and find that a 16-bit scaled integer model would suffice given the uncertainty of the unresolved sub-grid-scale dynamics. We then show that, by dedicating computational resources to spatial resolution rather than numeric precision in a field programmable gate array (FPGA), we see up to 28.6{\%} improvement in forecast accuracy, an approximately fivefold reduction in the number of logical computing elements required and an approximately 10-fold reduction in energy consumed by the FPGA, for the L96 model.},
author = {Jeffress, Stephen and D{\"{u}}ben, Peter and Palmer, Tim},
doi = {10.1098/rspa.2017.0144},
file = {:home/kloewer/papers/Jeffress, D{\"{u}}ben, Palmer{\_}2017.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
keywords = {atmospheric science,electrical engineering},
number = {2205},
pages = {20170144},
title = {{Bitwise efficiency in chaotic models}},
url = {http://rspa.royalsocietypublishing.org/lookup/doi/10.1098/rspa.2017.0144},
volume = {473},
year = {2017}
}
@article{Russell2017,
abstract = {Reconfigurable architectures are becoming mainstream: Amazon, Microsoft and IBM are supporting such architectures in their data centres. The computationally intensive nature of atmospheric modelling is an attractive target for hardware acceleration using reconfigurable computing. Performance of hardware designs can be improved through the use of reduced-precision arithmetic, but maintaining appropriate accuracy is essential. We explore reduced-precision optimisation for simulating chaotic systems, targeting atmospheric modelling, in which even minor changes in arithmetic behaviour will cause simulations to diverge quickly. The possibility of equally valid simulations having differing outcomes means that standard techniques for comparing numerical accuracy are inappropriate. We use the Hellinger distance to compare statistical behaviour between reduced-precision CPU implementations to guide reconfigurable designs of a chaotic system, then analyse accuracy, performance and power efficiency of the resulting implementations. Our results show that with only a limited loss in accuracy corresponding to less than 10{\%} uncertainty in input parameters, the throughput and energy efficiency of a single-precision chaotic system implemented on a Xilinx Virtex-6 SX475T Field Programmable Gate Array (FPGA) can be more than doubled.},
author = {Russell, Francis P. and D{\"{u}}ben, Peter D. and Niu, Xinyu and Luk, Wayne and Palmer, T. N.},
doi = {10.1016/j.cpc.2017.08.011},
file = {:home/kloewer/papers/Russell et al.{\_}2017.pdf:pdf},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Chaotic system,FPGA,Precision reduction,Weather modelling},
pages = {160--173},
publisher = {Elsevier B.V.},
title = {{Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures}},
url = {http://dx.doi.org/10.1016/j.cpc.2017.08.011},
volume = {221},
year = {2017}
}
@article{Tantet2018,
abstract = {Local bifurcations of stationary points and limit cycles have successfully been characterized in terms of the critical exponents of these solutions. Lyapunov exponents and their associated covariant Lyapunov vectors have been proposed as tools for supporting the understanding of critical transitions in chaotic dynamical systems. However, it is in general not clear how the statistical properties of dynamical systems change across a boundary crisis during which a chaotic attractor collides with a saddle. This behavior is investigated here for a boundary crisis in the Lorenz flow, for which neither the Lyapunov exponents nor the covariant Lyapunov vectors provide a criterion for the crisis. Instead, the convergence of the time evolution of probability densities to the invariant measure, governed by the semigroup of transfer operators, is expected to slow down at the approach of the crisis. Such convergence is described by the eigenvalues of the generator of this semigroup, which can be divided into two families, referred to as the stable and unstable Ruelle-Pollicott resonances, respectively. The former describes the convergence of densities to the attractor (or escape from a repeller) and is estimated from many short time series sampling the phase space. The latter is responsible for the decay of correlations, or mixing, and can be estimated from a long times series, invoking ergodicity. It is found numerically for the Lorenz flow that the stable resonances do approach the imaginary axis during the crisis, as is indicative of the loss of global stability of the attractor. On the other hand, the unstable resonances, and a fortiori the decay of correlations, do not ag the proximity of the crisis, thus questioning the usual design of early warning indicators of boundary crises of chaotic attractors and the applicability of response theory close to such crises.},
archivePrefix = {arXiv},
arxivId = {1705.08178},
author = {Tantet, Alexis and Lucarini, Valerio and Dijkstra, Henk A.},
doi = {10.1007/s10955-017-1938-0},
eprint = {1705.08178},
file = {:home/kloewer/papers/Tantet, Lucarini, Dijkstra{\_}2018.pdf:pdf},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Attractor crisis,Bifurcation,Ergodic theory,Resonance,Response theory,Transfer operator},
number = {3},
pages = {584--616},
publisher = {Springer US},
title = {{Resonances in a Chaotic Attractor Crisis of the Lorenz Flow}},
url = {https://doi.org/10.1007/s10955-017-1938-0},
volume = {170},
year = {2018}
}
@article{Chaurasiya2018,
author = {Chaurasiya, Rohit and Gustafson, John and Shrestha, Rahul and Neudorfer, Jonathan and Nambiar, Sangeeth and Niyogi, Kaustav},
file = {:home/kloewer/papers/Chaurasiya et al.{\_}2018.pdf:pdf},
pages = {9},
title = {{Parameterized Posit Arithmetic Hardware Generator}},
url = {https://posithub.org/docs/iccd_submission_v1.pdf},
year = {2018}
}
@article{Hatfield2018,
abstract = {Abstract The use of reduced numerical precision within an atmospheric data assimilation system is investigated. An atmospheric model with a spectral dynamical core is used to generate synthetic observations, which are then assimilated back into the same model using an ensemble Kalman filter. The effect on the analysis error of reducing precision from 64 bits to only 22 bits is measured and found to depend strongly on the degree of model uncertainty within the system. When the model used to generate the observations is identical to the model used to assimilate observations, the reduced-precision results suffer substantially. However, when model error is introduced by changing the diffusion scheme in the assimilation model or by using a higher-resolution model to generate observations, the difference in analysis quality between the two levels of precision is almost eliminated. Lower-precision arithmetic has a lower computational cost, so lowering precision could free up computational resources in operational data assimilation and allow an increase in ensemble size or grid resolution.},
author = {Hatfield, Sam and D{\"{u}}ben, Peter and Chantry, Matthew and Kondo, Keiichi and Miyoshi, Takemasa and Palmer, Tim},
doi = {10.1029/2018MS001341},
file = {:home/kloewer/papers/Hatfield et al.{\_}2018.pdf:pdf},
issn = {19422466},
journal = {Journal of Advances in Modeling Earth Systems},
keywords = {SPEEDY,data assimilation,ensemble Kalman filter,model error,reduced precision},
title = {{Choosing the Optimal Numerical Precision for Data Assimilation in the Presence of Model Error}},
year = {2018}
}
@article{Duben2014,
abstract = {A reduction of computational cost would allow higher resolution in numerical weather predictions within the same budget for computation. This paper investigates two approaches that promise significant savings in computational cost: the use of reduced precision hardware, which reduces floating point precision beyond the standard double- and single-precision arithmetic, and the use of stochastic processors, which allow hardware faults in a trade-off between reduced precision and savings in power consumption and computing time. Reduced precision is emulated within simulations of a spectral dynamical core of a global atmosphere model and a detailed study of the sensitivity of different parts of the model to inexact hardware is performed. Afterward, benchmark simulations were performed for which as many parts of the model as possible were put onto inexact hardware. Results show that large parts of the model could be integrated with inexact hardware at error rates that are surprisingly high or with reduced precision to only a couple of bits in the significand of floating point numbers. However, the sensitivities to inexact hardware of different parts of the model need to be respected, for example, via scale separation. In the last part of the paper, simulations with a full operational weather forecast model in single precision are presented. It is shown that differences in accuracy between the single- and double-precision forecasts are smaller than differences between ensemble members of the en- semble forecast at the resolution of the standard ensemble forecasting system. The simulations prove that the trade-off between precision and performance is a worthwhile effort, already on existing hardware.},
author = {D{\"{u}}ben, Peter D. and Palmer, T. N.},
doi = {10.1175/MWR-D-14-00110.1},
file = {:home/kloewer/papers/D{\"{u}}ben, Palmer{\_}2014.pdf:pdf},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {10},
pages = {3809--3829},
title = {{Benchmark Tests for Numerical Weather Forecasts on Inexact Hardware}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/MWR-D-14-00110.1},
volume = {142},
year = {2014}
}
@article{Thornes2017,
abstract = {Increasing the resolution of numerical models has played a large part in improving the accuracy of weather and climate forecasts in recent years. Until now, this has required the use of ever more powerful computers, the energy costs of which are becoming increasingly problematic. It has therefore been proposed that forecasters switch to using more efficient ‘reduced precision' hardware capable of sacrificing unnecessary numerical precision to save costs. Here, an extended form of the Lorenz ‘96 idealized model atmosphere is used to test whether more accurate forecasts could be produced by lowering numerical precision more at smaller spatial scales in order to increase the model resolution. Both a scale-dependent mixture of single- and half-precision – where numbers are represented with fewer bits of information on smaller spatial scales – and ‘stochastic processors' – where random ‘bit-flips' are allowed for small-scale variables – are emulated on conventional hardware. It is found that high-resolution parametrized models with scale-selective reduced precision yield better short-term and climatological forecasts than lower resolution parametrized models with conventional precision for a relatively small increase in computational cost. This suggests that a similar approach in real-world models could lead to more accurate and efficient weather and climate forecasts.},
author = {Thornes, Tobias and D{\"{u}}ben, Peter and Palmer, Tim},
doi = {10.1002/qj.2974},
file = {:home/kloewer/papers/Thornes, D{\"{u}}ben, Palmer{\_}2017.pdf:pdf},
issn = {1477870X},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {Lorenz ‘96,half precision,modelling,multiscale,predictability,reduced precision,stochastic parametrization},
number = {703},
pages = {897--908},
title = {{On the use of scale-dependent precision in Earth System modelling}},
volume = {143},
year = {2017}
}
@article{Dawson2017,
abstract = {{\textless}p{\textgreater}This paper describes the {\textless}i{\textgreater}rpe{\textless}/i{\textgreater} library which has the capability to emulate the use of arbitrary reduced floating-point precision within large numerical models written in Fortran. The {\textless}i{\textgreater}rpe{\textless}/i{\textgreater} software allows model developers to test how reduced floating-point precision affects the result of their simulations without having to make extensive code changes or port the model onto specialised hardware. The software can be used to identify parts of a program that are problematic for numerical precision and to guide changes to the program to allow a stronger reduction in precision. {\textless}br{\textgreater}{\textless}br{\textgreater} The development of {\textless}i{\textgreater}rpe{\textless}/i{\textgreater} was motivated by the strong demand for more computing power. If numerical precision can be reduced for an application under consideration while still achieving results of acceptable quality, computational cost can be reduced, since a reduction in numerical precision may allow an increase in performance or a reduction in power consumption. For simulations with weather and climate models, savings due to a reduction in precision could be reinvested to allow model simulations at higher spatial resolution or complexity, or to increase the number of ensemble members to improve predictions. {\textless}i{\textgreater}rpe{\textless}/i{\textgreater} was developed with particular focus on the community of weather and climate modelling, but the software could be used with numerical simulations from other domains.{\textless}/p{\textgreater}},
author = {Dawson, Andrew and D{\"{u}}ben, Peter D.},
doi = {10.5194/gmd-10-2221-2017},
file = {:home/kloewer/papers/Dawson, D{\"{u}}ben{\_}2017.pdf:pdf},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {6},
pages = {2221--2230},
title = {{Rpe v5: An emulator for reduced floating-point precision in large numerical simulations}},
volume = {10},
year = {2017}
}
@article{Palmer2015,
abstract = {Energy-optimized hybrid computers with a range of processor accuracies will advance modelling in fields from climate change to neuroscience},
author = {Palmer, Tim},
doi = {10.1038/526032a},
file = {:home/kloewer/papers/Palmer{\_}2015.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
pages = {2--3},
title = {{Build imprecise supercomputers}},
volume = {526},
year = {2015}
}
@unpublished{Gustafson2017b,
author = {Gustafson, John L},
file = {:home/kloewer/papers/Gustafson{\_}2017.pdf:pdf},
pages = {137},
title = {{Posit Arithmetic}},
url = {https://posithub.org/docs/Posits4.pdf},
year = {2017}
}
@article{Duben2018,
author = {D{\"{u}}ben, Peter D.},
doi = {10.1029/2018MS001420},
file = {:home/kloewer/papers/D{\"{u}}ben{\_}2018.pdf:pdf},
issn = {19422466},
journal = {Journal of Advances in Modeling Earth Systems},
keywords = {ensemble predictions,floating point numbers,high performance computing,information content,model development,predictability},
title = {{A new number format for ensemble simulations}},
url = {http://doi.wiley.com/10.1029/2018MS001420},
year = {2018}
}
@article{Gustafson2017,
abstract = {A new data type called a   posit   is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and “Not-a-Number” (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality.     A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than “approximate computing” methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases,   a 32-bit posit may safely replace a 64-bit float  . In other words, posits beat floats at their own game.},
author = {Gustafson, John L and Yonemoto, Isaac},
doi = {10.14529/jsfi170206},
file = {:home/kloewer/papers/Gustafson, Yonemoto{\_}2017.pdf:pdf},
issn = {23138734},
journal = {Supercomputing Frontiers and Innovations},
keywords = {1,arithmetic framework has several,background,computer arithmetic,energy-efficient computing,floating point,forms,ii unums,linear algebra,linpack,neural networks,posits,the original,the unum,type i and type,u niversal num ber,unum computing,valid arithmetic},
number = {2},
pages = {71--86},
title = {{Beating Floating Point at its Own Game: Posit Arithmetic}},
url = {http://superfri.org/superfri/article/view/137},
volume = {4},
year = {2017}
}
@article{Vana2017,
abstract = {AbstractEarth's climate is a nonlinear dynamical system with scale-dependent Lyapunov exponents. As such, an important theoretical question for modeling weather and climate is how much real information is carried in a model's physical variables as a function of scale and variable type. Answering this question is of crucial practical importance given that the development of weather and climate models is strongly constrained by available supercomputer power. As a starting point for answering this question, the impact of limiting almost all real-number variables in the forecasting mode of ECMWF Integrated Forecast System (IFS) from 64 to 32 bits is investigated. Results for annual integrations and medium-range ensemble forecasts indicate no noticeable reduction in accuracy, and an average gain in computational efficiency by approximately 40{\%}. This study provides the motivation for more scale-selective reductions in numerical precision.},
author = {V{\'{a}}{\v{n}}a, Filip and D{\"{u}}ben, Peter and Lang, Simon and Palmer, Tim and Leutbecher, Martin and Salmond, Deborah and Carver, Glenn},
doi = {10.1175/MWR-D-16-0228.1},
file = {:home/kloewer/papers/V{\'{a}}ňa et al.{\_}2017.pdf:pdf},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {2},
pages = {495--502},
title = {{Single Precision in Weather Forecasting Models: An Evaluation with the IFS}},
url = {http://journals.ametsoc.org/doi/10.1175/MWR-D-16-0228.1},
volume = {145},
year = {2017}
}
@software{Kloewer2019,
author = {Kl{\"{o}}wer, Milan and Giordano, Mose},
doi = {10.5281/zenodo.3590291},
publisher = {Zenodo},
title = {{SoftPosit.jl -  A posit arithmetic emulator}},
url = {https://doi.org/10.5281/zenodo.3590291},
year = {2019}
}
